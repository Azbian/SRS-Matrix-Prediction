{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def load_and_normalize_data(e2_path, srs_path, stats=None):\n",
    "    e2_data = np.load(e2_path)\n",
    "    srs_data = np.load(srs_path).astype(np.float32)\n",
    "\n",
    "    if stats is None:\n",
    "        srs_min = srs_data.min(axis=(0, 1), keepdims=True)\n",
    "        srs_max = srs_data.max(axis=(0, 1), keepdims=True)\n",
    "        stats = (srs_min, srs_max)\n",
    "        print(\"[INFO] Calculated training SRS normalization stats\")\n",
    "    else:\n",
    "        srs_min, srs_max = stats\n",
    "        print(\"[INFO] Using provided SRS normalization stats\")\n",
    "\n",
    "    srs_norm = (srs_data - srs_min) / (srs_max - srs_min + 1e-7)\n",
    "    return e2_data, srs_norm, stats\n",
    "\n",
    "def denormalize_srs(srs_norm, stats):\n",
    "    srs_min, srs_max = stats\n",
    "    return srs_norm * (srs_max - srs_min) + srs_min\n",
    "\n",
    "def get_fast_dataset(e2_data, srs_data, batch_size=4, e2_len=20,\n",
    "                     srs_input_rows=100, pred_offset=5):\n",
    "\n",
    "    num_steps_srs = (len(srs_data) - (srs_input_rows + pred_offset)) // 50\n",
    "    num_steps_e2 = len(e2_data) - e2_len\n",
    "    num_steps = min(num_steps_e2, num_steps_srs)\n",
    "\n",
    "    def data_generator():\n",
    "        for t in range(num_steps):\n",
    "            # ---- E2 ----\n",
    "            e2_chunk = e2_data[t:t + e2_len]\n",
    "            X_e2 = np.zeros(280, dtype=np.float32)\n",
    "            X_e2[:e2_chunk.size] = e2_chunk.flatten()[:280]\n",
    "            X_e2 = X_e2.reshape(5, 4, 14)\n",
    "\n",
    "            # ---- SRS input ----\n",
    "            srs_start = t * 50\n",
    "            X_srs = srs_data[srs_start:srs_start + srs_input_rows]\n",
    "            X_srs = X_srs.reshape(20, 20, 1536)\n",
    "\n",
    "            # ---- Label ----\n",
    "            y = srs_data[srs_start + srs_input_rows + pred_offset - 1]\n",
    "\n",
    "            yield (X_e2, X_srs), y\n",
    "\n",
    "    output_signature = (\n",
    "        (\n",
    "            tf.TensorSpec((5, 4, 14), tf.float32),\n",
    "            tf.TensorSpec((20, 20, 1536), tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec((4, 1536), tf.float32)\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.data.Dataset.from_generator(data_generator, output_signature=output_signature)\n",
    "        .cache()\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "def save_model_log(model, test_results, model_base_name=\"Channel_prediction\", log_file=\"logs/performance_log.txt\"):\n",
    "    \"\"\"\n",
    "    Checks the log file for the last version number, increments it, \n",
    "    and saves model metadata and evaluation results.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "\n",
    "    # --- 1. Identify the Next Model Number ---\n",
    "    next_version = 1\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            # Look specifically for lines containing your base name + a number\n",
    "            # Using regex to find the highest number logged so far\n",
    "            version_numbers = []\n",
    "            for line in lines:\n",
    "                if f\"MODEL NAME: {model_base_name}_\" in line:\n",
    "                    match = re.search(rf\"{model_base_name}_(\\d+)\", line)\n",
    "                    if match:\n",
    "                        version_numbers.append(int(match.group(1)))\n",
    "            \n",
    "            if version_numbers:\n",
    "                next_version = max(version_numbers) + 1\n",
    "\n",
    "    current_model_name = f\"{model_base_name}_{next_version}\"\n",
    "\n",
    "    # --- 2. Capture model summary internally ---\n",
    "    stream = StringIO()\n",
    "    model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "    model_summary_str = stream.getvalue()\n",
    "\n",
    "    # --- 3. Extract Metadata (Loss and Activation) ---\n",
    "    if hasattr(model, 'loss'):\n",
    "        loss_func = model.loss if isinstance(model.loss, str) else getattr(model.loss, '__name__', str(model.loss))\n",
    "    else:\n",
    "        loss_func = \"Unknown\"\n",
    "    \n",
    "    activation = \"None/Linear\"\n",
    "    for layer in reversed(model.layers):\n",
    "        if hasattr(layer, 'activation') and layer.activation is not None:\n",
    "            activation = layer.activation.__name__\n",
    "            break\n",
    "\n",
    "    # --- 4. Write data to the log file ---\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        f.write(f\"LOG TIME:          {datetime.now()}\\n\")\n",
    "        f.write(f\"MODEL NAME:        {current_model_name}\\n\")\n",
    "        f.write(f\"LOSS FUNCTION:     {loss_func}\\n\")\n",
    "        f.write(f\"OUTPUT ACTIVATION: {activation}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"===== MODEL SUMMARY =====\\n\")\n",
    "        f.write(model_summary_str + \"\\n\")\n",
    "        f.write(\"=========================\\n\\n\")\n",
    "        \n",
    "        f.write(\"===== TEST RESULTS =====\\n\")\n",
    "        for k, v in test_results.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "        f.write(\"=========================\\n\\n\")\n",
    "\n",
    "    print(f\"Logged as '{current_model_name}' to: {log_file}\")\n",
    "    return current_model_name\n",
    "\n",
    "e2_train, srs_train, train_stats = load_and_normalize_data(\"E2_train.npy\", \"SRS_train.npy\")\n",
    "train_ds = get_fast_dataset(e2_train, srs_train, batch_size=8) \n",
    "\n",
    "test_e2_npy = \"P:/SP Challenge/Model/E2_test.npy\"\n",
    "test_srs_npy = \"P:/SP Challenge/Model/SRS_test.npy\"\n",
    "\n",
    "test_e2, test_srs, test_stats = load_and_normalize_data(\n",
    "    test_e2_npy,\n",
    "    test_srs_npy\n",
    ")\n",
    "test_ds = get_fast_dataset(\n",
    "    test_e2,\n",
    "    test_srs,\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(e2_input_shape=(5, 4, 14), srs_input_shape=(20, 20, 1536), \n",
    "                 lstm_units=128, dropout_rate=0.2, stats=None):\n",
    "\n",
    "    # --- Radio Branch ---\n",
    "    radio_input = layers.Input(shape=e2_input_shape, name='radio_input')\n",
    "    r1 = layers.Normalization()(radio_input)\n",
    "    r1 = layers.Conv2D(32, (3, 3), padding='same')(r1)\n",
    "    r1 = layers.BatchNormalization()(r1)\n",
    "    r1 = layers.Activation('relu')(r1)\n",
    "    r1 = layers.ZeroPadding2D(padding=((0,0),(0,1)))(r1) \n",
    "    r1 = layers.Conv2D(64, (3, 3), padding='same')(r1)\n",
    "    r1 = layers.BatchNormalization()(r1)\n",
    "    r1 = layers.Activation('relu')(r1)\n",
    "    r1 = layers.Conv2D(128, (2, 2), padding='same')(r1)\n",
    "    r1 = layers.BatchNormalization()(r1)\n",
    "    r1 = layers.Activation('relu')(r1)\n",
    "    r1 = layers.Conv2D(256, (2, 2), padding='same')(r1)\n",
    "    r1 = layers.BatchNormalization()(r1)\n",
    "    r1 = layers.Activation('relu')(r1)\n",
    "\n",
    "    # --- SRS Branch ---\n",
    "    srs_input = layers.Input(shape=srs_input_shape, name='srs_input')\n",
    "    s1 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(srs_input)\n",
    "    \n",
    "    # Residual Block 1\n",
    "    s1_res1 = layers.Conv2D(256, (1, 1), padding='valid')(s1)\n",
    "    s1_res1 = layers.BatchNormalization()(s1_res1)\n",
    "    s1_res1 = layers.Activation('relu')(s1_res1)\n",
    "    s1_res1 = layers.Conv2D(256, (3, 3), padding='same')(s1_res1)\n",
    "    s1_res1 = layers.BatchNormalization()(s1_res1)\n",
    "    s1_res1 = layers.Activation('relu')(s1_res1)\n",
    "    s1_res1 = layers.Conv2D(512, (1, 1), padding='valid')(s1_res1)\n",
    "    s1_res1 = layers.BatchNormalization()(s1_res1)\n",
    "    \n",
    "    shortcut1 = layers.Conv2D(512, (1, 1), padding='valid')(s1)\n",
    "    shortcut1 = layers.BatchNormalization()(shortcut1)\n",
    "    \n",
    "    s1 = layers.Add()([s1_res1, shortcut1])\n",
    "    s1 = layers.Activation('relu')(s1)\n",
    "\n",
    "    # Residual Block 2\n",
    "    s1_res2 = layers.Conv2D(256, (1, 1), padding='valid')(s1)\n",
    "    s1_res2 = layers.BatchNormalization()(s1_res2)\n",
    "    s1_res2 = layers.Activation('relu')(s1_res2)\n",
    "    s1_res2 = layers.Conv2D(256, (3, 3), padding='same')(s1_res2)\n",
    "    s1_res2 = layers.BatchNormalization()(s1_res2)\n",
    "    s1_res2 = layers.Activation('relu')(s1_res2)\n",
    "    s1_res2 = layers.Conv2D(512, (1, 1), padding='valid')(s1_res2)\n",
    "    s1_res2 = layers.BatchNormalization()(s1_res2)\n",
    "    \n",
    "    shortcut2 = layers.Conv2D(512, (1, 1), padding='valid')(s1)\n",
    "    shortcut2 = layers.BatchNormalization()(shortcut2)\n",
    "    \n",
    "    s1 = layers.Add()([s1_res2, shortcut2])\n",
    "    s1 = layers.Activation('relu')(s1)\n",
    "\n",
    "    s1 = layers.AveragePooling2D((2, 2))(s1)\n",
    "    s1 = layers.AveragePooling2D((2, 2))(s1)\n",
    "\n",
    "    # --- Fusion & Output ---\n",
    "    x = layers.Concatenate(axis=-1)([r1, s1]) \n",
    "    x = layers.Conv2D(1024, (1, 1), activation='relu', padding='same')(x)\n",
    "    x = layers.Reshape((25, 1024))(x)\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(2048, activation='relu')(x)\n",
    "    \n",
    "    # This is the normalized prediction\n",
    "    x = layers.Dense(4 * 1536, activation='linear')(x)\n",
    "    x = layers.Reshape((4, 1536))(x)\n",
    "\n",
    "    # --- Denormalization Layer ---\n",
    "    if stats is not None:\n",
    "        srs_min, srs_max = stats\n",
    "        # Converting stats to constants within the graph\n",
    "        srs_min = tf.constant(srs_min, dtype=tf.float32)\n",
    "        srs_max = tf.constant(srs_max, dtype=tf.float32)\n",
    "        \n",
    "        # Denormalization: (x * (max - min)) + min\n",
    "        output = layers.Lambda(lambda val: val * (srs_max - srs_min) + srs_min, name=\"denorm_output\")(x)\n",
    "    else:\n",
    "        output = layers.Activation('linear', name=\"output\")(x)\n",
    "\n",
    "    model = models.Model(inputs=[radio_input, srs_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='log_cosh', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(stats=train_stats)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8686ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=100)\n",
    "\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ---- Loss (log-cosh) ----\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['loss'], 'bo-', label='Training Loss (log-cosh)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ---- MSE ----\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['mse'], 'ro-', label='Training MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Training MSE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "results = model.evaluate(test_ds, return_dict=True)\n",
    "\n",
    "print(\"Test Loss:\", results['loss'])\n",
    "print(\"Test MSE :\", results['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_log(model, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB', # 'TB' for top-to-bottom, 'LR' for left-to-right\n",
    "    expand_nested=True,\n",
    "    dpi=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92155fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_save_path = \"P:/SP Challenge/Model/Saved/model_weights.h5\"\n",
    "model.save_weights(weights_save_path)\n",
    "print(f\"Model weights saved to {weights_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
