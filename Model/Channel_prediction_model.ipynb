{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# --- 1. PRE-PROCESSING FUNCTION (Run once) ---\n",
    "def convert_csv_to_numpy(e2_path, srs_path, output_prefix):\n",
    "    \"\"\"Parses raw CSVs into efficient NumPy binaries.\"\"\"\n",
    "    print(f\"Converting {e2_path} and {srs_path} to binary format...\")\n",
    "    \n",
    "    # Process E2\n",
    "    df_e2 = pd.read_csv(e2_path, header=None).apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    e2_data = df_e2.iloc[:, :19].values.astype(np.float32)\n",
    "    \n",
    "    # Process SRS (The slow part)\n",
    "    df_srs = pd.read_csv(srs_path, header=None)\n",
    "    parsed_srs = []\n",
    "    for val in df_srs[0]:\n",
    "        # Parse string once and store as float32\n",
    "        raw_arr = np.array(ast.literal_eval(val), dtype=np.float32)\n",
    "        parsed_srs.append(raw_arr.T.reshape(4, 1536))\n",
    "    srs_data = np.stack(parsed_srs, axis=0)\n",
    "    \n",
    "    # Save to disk\n",
    "    np.save(f\"{output_prefix}_e2.npy\", e2_data)\n",
    "    np.save(f\"{output_prefix}_srs.npy\", srs_data)\n",
    "    print(\"Pre-processing complete.\")\n",
    "\n",
    "# --- 2. OPTIMIZED DATA PIPELINE ---\n",
    "def get_fast_dataset(e2_npy, srs_npy, batch_size=4, e2_len=20, srs_input_rows=100, pred_offset=50):\n",
    "    # Load the binary data\n",
    "    e2_data = np.load(e2_npy)\n",
    "    srs_data = np.load(srs_npy)\n",
    "\n",
    "    # Calculate valid steps\n",
    "    num_steps_srs = (len(srs_data) - (srs_input_rows + pred_offset)) // 50\n",
    "    num_steps_e2 = len(e2_data) - e2_len\n",
    "    num_steps = min(num_steps_e2, num_steps_srs)\n",
    "\n",
    "    def data_generator():\n",
    "        for t in range(num_steps):\n",
    "            # E2 Processing\n",
    "            e2_chunk = e2_data[t : t + e2_len]\n",
    "            # Fast flatten and reshape\n",
    "            X_e2 = np.zeros(380, dtype=np.float32)\n",
    "            X_e2[:e2_chunk.size] = e2_chunk.flatten()[:380]\n",
    "            X_e2 = X_e2.reshape(5, 4, 19)\n",
    "\n",
    "            # SRS Processing (Jumping by 50)\n",
    "            srs_start = t * 50\n",
    "            X_srs = srs_data[srs_start : srs_start + srs_input_rows].reshape(20, 20, 1536)\n",
    "\n",
    "            # Label\n",
    "            y = srs_data[srs_start + srs_input_rows + pred_offset - 1]\n",
    "            \n",
    "            yield (X_e2, X_srs), y\n",
    "\n",
    "    output_signature = (\n",
    "        (tf.TensorSpec(shape=(5, 4, 19), dtype=tf.float32), \n",
    "         tf.TensorSpec(shape=(20, 20, 1536), dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(4, 1536), dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(data_generator, output_signature=output_signature)\n",
    "\n",
    "    return ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(e2_input_shape=(5, 4, 19), srs_input_shape=(20, 20, 1536), lstm_units=128, dropout_rate=0.3):\n",
    "    # Radio Branch\n",
    "    radio_input = layers.Input(shape=e2_input_shape, name='radio_input')\n",
    "    r1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(radio_input)\n",
    "    r1 = layers.BatchNormalization()(r1)\n",
    "    r1 = layers.ZeroPadding2D(padding=((0,0),(0,1)))(r1) # Result: (5, 5, 32)\n",
    "\n",
    "    # SRS Branch\n",
    "    srs_input = layers.Input(shape=srs_input_shape, name='srs_input')\n",
    "    s1 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(srs_input) # Reduced filters for speed\n",
    "    s1 = layers.MaxPooling2D((2, 2))(s1) # 10x10\n",
    "    s1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(s1)\n",
    "    s1 = layers.MaxPooling2D((2, 2))(s1) # 5x5\n",
    "    s1 = layers.BatchNormalization()(s1)\n",
    "\n",
    "    # Concatenate\n",
    "    x = layers.Concatenate(axis=-1)([r1, s1]) # (5, 5, 32 + 64)\n",
    "    x = layers.Reshape((25, 96))(x)\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True)(x)\n",
    "    x = layers.Flatten()(x) \n",
    "    x = layers.Dense(4 * 1536)(x)\n",
    "    output = layers.Reshape((4, 1536))(x)\n",
    "\n",
    "    model = models.Model(inputs=[radio_input, srs_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_E2 = \"P:/SP Challenge/DataSet/Preprocessed Dataset/combined_E2.csv\"\n",
    "TRAIN_SRS = \"P:/SP Challenge/DataSet/Preprocessed Dataset/combined_pp_srs.csv\"\n",
    "\n",
    "# Pre-process once\n",
    "if not os.path.exists(\"train_e2.npy\"):\n",
    "    convert_csv_to_numpy(TRAIN_E2, TRAIN_SRS, \"train\")\n",
    "\n",
    "# Create optimized dataset\n",
    "train_ds = get_fast_dataset(\"train_e2.npy\", \"train_srs.npy\", batch_size=16) # Increased batch size\n",
    "\n",
    "# Model and training\n",
    "model = create_model()\n",
    "model.fit(train_ds, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d2350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test binaries for test_file_1...\n",
      "Converting P:/SP Challenge/DataSet/Preprocessed Dataset/E2_test.csv and P:/SP Challenge/DataSet/Preprocessed Dataset/pp_srs_test.csv to binary format...\n",
      "Pre-processing complete.\n",
      "49/49 [==============================] - 1s 9ms/step - loss: 6969.4121\n",
      "Test MSE: 6969.412109375\n",
      "49/49 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_fast_test_dataset(e2_npy, srs_npy, e2_len=20, srs_input_rows=100, pred_offset=50):\n",
    "    # Load binary test data\n",
    "    e2_data = np.load(e2_npy)\n",
    "    srs_data = np.load(srs_npy)\n",
    "\n",
    "    num_steps_srs = (len(srs_data) - (srs_input_rows + pred_offset)) // 50\n",
    "    num_steps_e2 = len(e2_data) - e2_len\n",
    "    num_steps = min(num_steps_e2, num_steps_srs)\n",
    "\n",
    "    def test_data_generator():\n",
    "        for t in range(num_steps):\n",
    "            # E2 Window\n",
    "            e2_chunk = e2_data[t : t + e2_len]\n",
    "            X_e2 = np.zeros(380, dtype=np.float32)\n",
    "            X_e2[:e2_chunk.size] = e2_chunk.flatten()[:380]\n",
    "            X_e2 = X_e2.reshape(5, 4, 19)\n",
    "\n",
    "            # SRS Window (Jumping by 50)\n",
    "            srs_start = t * 50\n",
    "            X_srs = srs_data[srs_start : srs_start + srs_input_rows].reshape(20, 20, 1536)\n",
    "\n",
    "            # Ground Truth Label\n",
    "            y_true = srs_data[srs_start + srs_input_rows + pred_offset - 1]\n",
    "            \n",
    "            yield (X_e2, X_srs), y_true\n",
    "\n",
    "    output_signature = (\n",
    "        (tf.TensorSpec(shape=(5, 4, 19), dtype=tf.float32), \n",
    "         tf.TensorSpec(shape=(20, 20, 1536), dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(4, 1536), dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    # For testing/prediction, we use batch(1) as you did originally\n",
    "    return tf.data.Dataset.from_generator(test_data_generator, output_signature=output_signature).batch(1)\n",
    "\n",
    "test_e2_npy, test_srs_npy = (\"P:/SP Challenge/Model/train_e2.npy\", \"P:/SP Challenge/Model/train_srs.npy\")\n",
    "\n",
    "\n",
    "test_ds = get_fast_test_dataset(test_e2_npy, test_srs_npy)\n",
    "\n",
    "\n",
    "test_loss = model.evaluate(test_ds)\n",
    "print(f\"Test MSE: {test_loss}\")\n",
    "\n",
    "\n",
    "predictions = model.predict(test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
