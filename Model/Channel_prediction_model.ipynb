{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adea31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# --- 1. The Multi-Rate Sliding Window Generator ---\n",
    "def combined_generator(e2_csv, srs_csv, e2_len=20, srs_input_rows=100, pred_offset=50):\n",
    "    # Load E2\n",
    "    df_e2 = pd.read_csv(e2_csv, header=None).apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "    # Load and Parse SRS\n",
    "    df_srs = pd.read_csv(srs_csv, header=None)\n",
    "    parsed_srs = []\n",
    "    for val in df_srs[0]:\n",
    "        raw_arr = np.array(ast.literal_eval(val), dtype=np.float32)\n",
    "        arr_reshaped = raw_arr.T.reshape(4, 1536)\n",
    "        parsed_srs.append(arr_reshaped)\n",
    "    srs_data = np.stack(parsed_srs, axis=0)\n",
    "\n",
    "    # We determine steps based on the SRS file because it's the jumping one\n",
    "    # If SRS jumps by 50, we can only take as many steps as SRS allows\n",
    "    total_srs_needed_per_step = 100 + pred_offset # 150\n",
    "    # max_t * 50 + 150 <= total_srs\n",
    "    num_steps_srs = (len(srs_data) - total_srs_needed_per_step) // 50\n",
    "\n",
    "    # Also check E2 length\n",
    "    num_steps_e2 = len(df_e2) - e2_len\n",
    "\n",
    "    num_steps = min(num_steps_e2, num_steps_srs)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        # --- E2 Input: Slides 1 by 1 ---\n",
    "        e2_chunk = df_e2.iloc[t : t + e2_len, :19].values\n",
    "        flat_e2 = np.zeros(5 * 4 * 19, dtype=np.float32)\n",
    "        flat_e2[:min(e2_chunk.size, 380)] = e2_chunk.flatten()[:380]\n",
    "        X_e2 = flat_e2.reshape(5, 4, 19)\n",
    "\n",
    "        # --- SRS Input: Jumps by 50 ---\n",
    "        srs_start = t * 50\n",
    "        srs_end = srs_start + srs_input_rows\n",
    "        X_srs_window = srs_data[srs_start : srs_end] # (100, 4, 1536)\n",
    "        X_srs = X_srs_window.reshape(20, 20, 1536)\n",
    "\n",
    "        # --- Label: 50 steps after the SRS window ends ---\n",
    "        label_idx = srs_end + pred_offset - 1\n",
    "        y = srs_data[label_idx]\n",
    "\n",
    "        yield (X_e2, X_srs), y\n",
    "\n",
    "def test_generator(e2_test_path, srs_test_path, e2_len=20, srs_input_rows=100, pred_offset=50):\n",
    "    # Load Test Data\n",
    "    df_e2 = pd.read_csv(e2_test_path, header=None).apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    df_srs = pd.read_csv(srs_test_path, header=None)\n",
    "    \n",
    "    parsed_srs = []\n",
    "    for val in df_srs[0]:\n",
    "        raw_arr = np.array(ast.literal_eval(val), dtype=np.float32)\n",
    "        parsed_srs.append(raw_arr.T.reshape(4, 1536))\n",
    "    srs_data = np.stack(parsed_srs, axis=0)\n",
    "\n",
    "    num_steps = min(len(df_e2) - e2_len, (len(srs_data) - (100 + pred_offset)) // 50)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        # E2 Window\n",
    "        e2_chunk = df_e2.iloc[t : t + e2_len, :19].values\n",
    "        X_e2 = np.zeros(380, dtype=np.float32)\n",
    "        X_e2[:e2_chunk.size] = e2_chunk.flatten()[:380]\n",
    "        X_e2 = X_e2.reshape(5, 4, 19)\n",
    "\n",
    "        # SRS Window (Jumping by 50)\n",
    "        srs_start = t * 50\n",
    "        X_srs = srs_data[srs_start : srs_start + 100].reshape(20, 20, 1536)\n",
    "\n",
    "        # Ground Truth Label\n",
    "        y_true = srs_data[srs_start + 100 + pred_offset - 1]\n",
    "\n",
    "        yield (X_e2, X_srs), y_true\n",
    "\n",
    "def get_dataset(e2_list, srs_list, batch_size=4):\n",
    "    output_signature = (\n",
    "        (\n",
    "            tf.TensorSpec(shape=(5, 4, 19), dtype=tf.float32), \n",
    "            tf.TensorSpec(shape=(20, 20, 1536), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(4, 1536), dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: combined_generator(e2_list, srs_list),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    # NO SHUFFLE - maintains the exact timeline of the CSVs\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f73f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_model(e2_input_shape=(5, 4, 19), srs_input_shape=(20, 20, 1536), lstm_units=128, dropout_rate=0.3):\n",
    "    \n",
    "    radio_input = layers.Input(shape=e2_input_shape, name='radio_input')\n",
    "    \n",
    "    r1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(radio_input)  # use padded r1\n",
    "    r1 = layers.BatchNormalization()(r1)\n",
    "    r1 = layers.Conv2D(32, (2, 2), activation='relu', padding='same')(r1)\n",
    "    r1 = layers.BatchNormalization()(r1)\n",
    "    r1 = layers.Dropout(dropout_rate)(r1)\n",
    "    r1 = layers.ZeroPadding2D(padding=((0,0),(0,1)))(r1)  # pad width by 1\n",
    "\n",
    "    srs_input = layers.Input(shape=srs_input_shape, name='srs_input')\n",
    "    s1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(srs_input)\n",
    "    s1 = layers.Conv2D(1024, (1, 1), activation='relu', padding='same')(s1)\n",
    "    s1 = layers.BatchNormalization()(s1)\n",
    "    s1 = layers.MaxPooling2D((2, 2))(s1)\n",
    "    s1 = layers.Conv2D(32, (2, 2), activation='relu', padding='same')(s1)\n",
    "    s1 = layers.Conv2D(256, (1, 1), activation='relu', padding='same')(s1)\n",
    "    s1 = layers.Conv2D(128, (2, 2), activation='relu', padding='same')(s1)\n",
    "    s1 = layers.BatchNormalization()(s1)\n",
    "    s1 = layers.MaxPooling2D((2, 2))(s1)\n",
    "    s1 = layers.Conv2D(128, (1, 1), activation='relu', padding='same')(s1)\n",
    "    s1 = layers.Conv2D(64, (1, 1), activation='relu', padding='same')(s1)\n",
    "    s1 = layers.Dropout(dropout_rate)(s1)\n",
    "\n",
    "    # Concatenate and LSTM\n",
    "    x = layers.Concatenate(axis=-1)([r1, s1])\n",
    "    x = layers.Reshape((5*5, 32+64))(x)\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True)(x)\n",
    "    x = layers.Flatten()(x)         \n",
    "    x = layers.Dense(4*1536)(x)      \n",
    "    output = layers.Reshape((4,1536))(x)\n",
    "\n",
    "    model = models.Model(inputs=[radio_input, srs_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b4623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " srs_input (InputLayer)         [(None, 20, 20, 153  0           []                               \n",
      "                                6)]                                                               \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 20, 20, 64)   884800      ['srs_input[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 20, 20, 1024  66560       ['conv2d_11[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 20, 20, 1024  4096       ['conv2d_12[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 1024  0          ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 10, 10, 32)   131104      ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " radio_input (InputLayer)       [(None, 5, 4, 19)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 10, 10, 256)  8448        ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 5, 4, 64)     11008       ['radio_input[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 10, 10, 128)  131200      ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 5, 4, 64)    256         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 10, 10, 128)  512        ['conv2d_15[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 5, 4, 32)     8224        ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 128)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 5, 4, 32)    128         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 5, 5, 128)    16512       ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5, 4, 32)     0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 5, 5, 64)     8256        ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 5, 5, 32)    0           ['dropout_2[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5, 5, 64)     0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 5, 5, 96)     0           ['zero_padding2d_1[0][0]',       \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 25, 96)       0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 25, 128)      115200      ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 3200)         0           ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6144)         19666944    ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 4, 1536)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,053,248\n",
      "Trainable params: 21,050,752\n",
      "Non-trainable params: 2,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  ValueError: malformed node or string: nan\nTraceback (most recent call last):\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\mdraf\\AppData\\Local\\Temp\\ipykernel_38520\\3896327650.py\", line 16, in combined_generator\n    raw_arr = np.array(ast.literal_eval(val), dtype=np.float32)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 110, in literal_eval\n    return _convert(node_or_string)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 109, in _convert\n    return _convert_signed_num(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 83, in _convert_signed_num\n    return _convert_num(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 74, in _convert_num\n    _raise_malformed_node(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 71, in _raise_malformed_node\n    raise ValueError(msg + f': {node!r}')\n\nValueError: malformed node or string: nan\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_4]]\n  (1) INVALID_ARGUMENT:  ValueError: malformed node or string: nan\nTraceback (most recent call last):\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\mdraf\\AppData\\Local\\Temp\\ipykernel_38520\\3896327650.py\", line 16, in combined_generator\n    raw_arr = np.array(ast.literal_eval(val), dtype=np.float32)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 110, in literal_eval\n    return _convert(node_or_string)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 109, in _convert\n    return _convert_signed_num(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 83, in _convert_signed_num\n    return _convert_num(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 74, in _convert_num\n    _raise_malformed_node(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 71, in _raise_malformed_node\n    raise ValueError(msg + f': {node!r}')\n\nValueError: malformed node or string: nan\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_9101]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP:/SP Challenge/DataSet/Preprocessed Dataset/E2_1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      2\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP:/SP Challenge/DataSet/Preprocessed Dataset/pp_srs_1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m                         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  ValueError: malformed node or string: nan\nTraceback (most recent call last):\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\mdraf\\AppData\\Local\\Temp\\ipykernel_38520\\3896327650.py\", line 16, in combined_generator\n    raw_arr = np.array(ast.literal_eval(val), dtype=np.float32)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 110, in literal_eval\n    return _convert(node_or_string)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 109, in _convert\n    return _convert_signed_num(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 83, in _convert_signed_num\n    return _convert_num(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 74, in _convert_num\n    _raise_malformed_node(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 71, in _raise_malformed_node\n    raise ValueError(msg + f': {node!r}')\n\nValueError: malformed node or string: nan\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_4]]\n  (1) INVALID_ARGUMENT:  ValueError: malformed node or string: nan\nTraceback (most recent call last):\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\mdraf\\AppData\\Local\\Temp\\ipykernel_38520\\3896327650.py\", line 16, in combined_generator\n    raw_arr = np.array(ast.literal_eval(val), dtype=np.float32)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 110, in literal_eval\n    return _convert(node_or_string)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 109, in _convert\n    return _convert_signed_num(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 83, in _convert_signed_num\n    return _convert_num(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 74, in _convert_num\n    _raise_malformed_node(node)\n\n  File \"c:\\Users\\mdraf\\anaconda3\\envs\\py310\\lib\\ast.py\", line 71, in _raise_malformed_node\n    raise ValueError(msg + f': {node!r}')\n\nValueError: malformed node or string: nan\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_9101]"
     ]
    }
   ],
   "source": [
    "train_ds = get_dataset(\"P:/SP Challenge/DataSet/Preprocessed Dataset/combined_E2.csv\", \n",
    "                        \"P:/SP Challenge/DataSet/Preprocessed Dataset/combined_pp_srs.csv\",\n",
    "                        batch_size=4)\n",
    "model.fit(train_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d2350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 116s 8ms/step - loss: 6432.0234\n",
      "Test MSE: 6432.0234375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: test_generator(\"P:/SP Challenge/DataSet/Preprocessed Dataset/E2_1.csv\", \n",
    "                           \"P:/SP Challenge/DataSet/Preprocessed Dataset/pp_srs_1.csv\"),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(5, 4, 19), dtype=tf.float32), \n",
    "         tf.TensorSpec(shape=(20, 20, 1536), dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(4, 1536), dtype=tf.float32)\n",
    "    )\n",
    ").batch(1)\n",
    "\n",
    "test_loss = model.evaluate(test_ds)\n",
    "print(f\"Test MSE: {test_loss}\")\n",
    "\n",
    "predictions = model.predict(test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
